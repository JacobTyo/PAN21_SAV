{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "scenic-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer, InputExample, models, losses\n",
    "from torch import nn\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pleasant-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, build a dataloader for the IMDB62 dataset \n",
    "class IMDB62_AV_Dataset(Dataset):\n",
    "    \"\"\"Dataset for Author Verification on the IMDB62 Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_file, dataset_size=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_file (string): the path to the IMDB62 Dataset txt file\n",
    "        \"\"\"\n",
    "        self.data_file = data_file\n",
    "        self.dataset_size = dataset_size\n",
    "        \n",
    "        # read the file into a convenient data structure - needs to be of the form (text1, text2, similarity)\n",
    "        raw_data = []  # This will just store (userid, content) tuples\n",
    "        with open(self.data_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if self.dataset_size is None:\n",
    "                self.dataset_size = len(lines)\n",
    "            else:\n",
    "                assert self.dataset_size > len(lines), f'The desired dataset size must be larger than the actual dataset size which is: {len(lines)}'\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.split('\\t')\n",
    "                user_id = line[1]\n",
    "                text = line[-1]\n",
    "                raw_data.append((user_id, text))\n",
    "                    \n",
    "        # now process the individual files into an actual dataset\n",
    "        # random sampling should work fine for now, but first do a pass through the data \n",
    "        # with (n, n+1, label) to ensure that everything is seen at least once\n",
    "        self.data = []\n",
    "        for i in range(len(raw_data) - 1):\n",
    "            dp1 = raw_data[i]\n",
    "            dp2 = raw_data[i+1]\n",
    "            label = float(1) if dp1[0] == dp2[0] else float(0)\n",
    "            self.data.append(InputExample(texts=[dp1[1], dp2[1]], label=label))\n",
    "        \n",
    "        # now randomly sample to increase dataset size - there are 2^62,000 combinations, so just randomly sample like, 100k or so and call it good\n",
    "        for i in range(self.dataset_size - len(self.data)):\n",
    "            dp1 = random.choice(raw_data)\n",
    "            dp2 = random.choice(raw_data)\n",
    "            # make sure points aren't the same\n",
    "            while (dp1[0] == dp2[0] and dp1[1] == dp2[1]):\n",
    "                dp1 = random.choice(raw_data)\n",
    "                dp2 = random.choice(raw_data)\n",
    "            # add to the dataset\n",
    "            label = float(1) if dp1[0] == dp2[0] else float(0)\n",
    "            self.data.append(InputExample(texts=[dp1[1], dp2[1]], label=label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "given-yeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "<InputExample> label: 1.0, texts: I caught glimpses of this show which feature a gay male couple and their dog , Liberace . First of all , they come across as the most stereotyped gay couple that I have seen and yet they are real . I couldn't watch them with their dog as they took this tiny lap dog ( I mean the dog weighed under 10 pounds and was not fully developed ) and pushed into these contests . I thought Showbiz Moms and Dads were ridiculous but this young gay couple are obviously immature and neglectful of Liberace . I understand that some people want to place their animals for show but don't put Liberace on for show like a doll . Liberace is a dog and a small one . I had a rabbit who weighed more than this dog and I wouldn't put it in contests . Liberace isn't even attractive . Look I know people love their animals , I still can't get over the loss of my rabbit but even I have a good sense about what she wanted . I don't think Liberace wants to be paraded around like show dog . He probably just wants to sleep and then when they started going on about his penal development , I just cringed and prayed for somebody like the PETA people to see this as animal abuse and cruelty . If they don't conclude that , I don't know what else is my opinion . Poor Liberace , he is the one suffering the most . As for his owners , if they want to parade dogs , they should get bigger sized dogs which show interest in performing .\n",
      "; Believe it or not , I watched this show in the beginning in 1996 and I was a fan of it then . Not anymore , I grew to lose respect for all of the co-hosts at one time or another . Never did I think that Meredith Viera would announce to the world that she is not wearing underwear . Debbie Matenopoulos was far better than her replacements despite her sometimes idiotic comments . Until yesterday when Star Jones Reynolds announced her departure from the show did it hit me , I can't stand Rosie O'Donnell to begin with because she is a lying hypocrite of a human being and for her to succeed Viera's departure only supports my decision to have stopped watching . Who wants to see Rosie O'Donnell again as a talk show host anyway ? That was what was behind Star's departure was the Rosie's arrival . Not that Star is herself blameless , she has changed a lot since her surgery and her marriage but she managed to maintain some of my respect . After all , she is a lawyer and I don't think she's no dummy . Barbara Walters , what have you done to this show since it first aired 9 years ago . I could see why Meredith is leaving for better opportunities . I can't stand Joy Behar anymore who I used to enjoy watching as a comedian . I had respect for Star Jones until this show and watched it deplete over the years . Debbie's replacements have never had the same magic as the show once did when it premiered and even less when Rosie joins the show . With Star , I wish her and Meredith the best . But come on , Barbara , you can go outside and get somebody off the streets of New York City who doesn't talk English and do a better job then Rosie . They could be homeless , drunk , and wasted and I bet they would be funnier and more original than Rosie O'Donnell could ever be . I was reading that you were going to ask Marcia Cross about those lesbian rumors in front of her parents even though she was engaged to a man at the time but Marcia's quite a lady but you lost her respect . How many other people's respect are you going to lose now . Maybe Barbara should retire herself . Now if Barbara was smart , she would have gotten Kathie Lee Gifford to take over Meredith's spot . I plan on watching something less argumentative , maybe I'll switch over to Jerry Springer from now on .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the dataset \n",
    "\n",
    "train_dataset = IMDB62_AV_Dataset('/home/jtyo/Projects/Authorship Attribution/IMDB/imdb62_train.txt', 200_000)\n",
    "test_dataset = IMDB62_AV_Dataset('/home/jtyo/Projects/Authorship Attribution/IMDB/imdb62_test.txt')\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "remarkable-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now train a model \n",
    "word_embedding_model = models.Transformer('bert-base-cased', max_seq_length=512)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.ReLU())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "respective-jacket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0a446d8cbb4489b44796cc9388dbf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f9152bb41a4e5eb2b911f111715042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-757cb758b75c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Tune the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_objectives\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Apps/Anaconda/anaconda3/envs/aa/lib/python3.7/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, output_path_ignore_not_empty)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m                         \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Apps/Anaconda/anaconda3/envs/aa/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Apps/Anaconda/anaconda3/envs/aa/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# build dataloader from the dataset \n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)\n",
    "\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=5, warmup_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-catering",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
